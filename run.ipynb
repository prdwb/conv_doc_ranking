{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from pytorch_transformers import (WEIGHTS_NAME, BertConfig, BertTokenizer)\n",
    "from modeling import (BertConcatForStatefulSearch, \n",
    "                      BehaviorAwareBertConcatForStatefulSearch, \n",
    "                      HierBertConcatForStatefulSearch, \n",
    "                      HierAttBertConcatForStatefulSearch, \n",
    "                      BehaviorAwareHierAttBertConcatForStatefulSearch)\n",
    "\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from utils import (compute_metrics, convert_examples_to_features, output_modes, ConcatModelDataset)\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertConcatForStatefulSearch, BertTokenizer),\n",
    "    'ba_bert': (BertConfig, BehaviorAwareBertConcatForStatefulSearch, BertTokenizer),\n",
    "    'hier': (BertConfig, HierBertConcatForStatefulSearch, BertTokenizer),\n",
    "    'hier_att': (BertConfig, HierAttBertConcatForStatefulSearch, BertTokenizer),\n",
    "    'ba_hier_att': (BertConfig, BehaviorAwareHierAttBertConcatForStatefulSearch, BertTokenizer),\n",
    "}\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        \n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, eval_dataset, model, tokenizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter(os.path.join(args.output_dir, 'logs'))\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, \n",
    "                                  batch_size=args.train_batch_size, num_workers=args.num_workers)\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    # no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon, correct_bias=True)\n",
    "    args.warmup_steps = int(t_total * args.warmup_portion)\n",
    "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                          output_device=args.local_rank,\n",
    "                                                          find_unused_parameters=True)\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                   args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    \n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
    "    set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            guids = batch['guid']\n",
    "            batch = {k: v.to(args.device) for k, v in batch.items() if k != 'guid'}\n",
    "            inputs = {'input_ids':      batch['input_ids'],\n",
    "                      'attention_mask': batch['input_mask'],\n",
    "                      'token_type_ids': batch['segment_ids'],\n",
    "                      'labels':         batch['ranker_label_ids']}\n",
    "            if args.model_type == 'hier':\n",
    "                inputs['hier_mask'] = batch['hier_mask']\n",
    "            elif args.model_type in ['ba_bert', 'hier_att', 'ba_hier_att']:\n",
    "                inputs['hier_mask'] = batch['hier_mask']\n",
    "                inputs['behavior_rel_pos_mask'] = batch['behavior_rel_pos_mask']\n",
    "                inputs['behavior_type_mask'] = batch['behavior_type_mask']\n",
    "                # print('hier_mask', batch['hier_mask'].tolist())\n",
    "                # print('behavior_rel_pos_mask', batch['behavior_rel_pos_mask'].tolist())\n",
    "                # print('behavior_type_mask', batch['behavior_type_mask'].tolist())\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                \n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics                   \n",
    "                    tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    # Save model checkpoint if it outperforms previous models\n",
    "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "                    \n",
    "                    \n",
    "#                     if args.local_rank == -1 and args.evaluate_during_training:\n",
    "#                         results, eval_output = evaluate(args, eval_dataset, model, \n",
    "#                                                     tokenizer, args.per_gpu_eval_batch_size)\n",
    "#                         for key, value in results.items():\n",
    "#                             tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
    "\n",
    "#                     if results['mrr'] > best_eval_mrr:\n",
    "#                         best_eval_mrr = results['mrr']\n",
    "#                         output_dir = os.path.join(args.output_dir, 'checkpoint')\n",
    "#                         if not os.path.exists(output_dir):\n",
    "#                             os.makedirs(output_dir)\n",
    "#                         model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "#                         model_to_save.save_pretrained(output_dir)\n",
    "#                         torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
    "#                         logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "#                         output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "#                         with open(output_eval_file, \"w\") as writer:\n",
    "#                             logger.info(\"***** Best eval results so far *****\")\n",
    "#                             for key in sorted(results.keys()):\n",
    "#                                 logger.info(\"  %s = %s\", key, str(results[key]))\n",
    "#                                 writer.write(\"%s = %s\\n\" % (key, str(results[key])))\n",
    "\n",
    "#                         output_eval_preds_file = os.path.join(args.output_dir, \"eval_preds.txt\")\n",
    "#                         with open(output_eval_preds_file, 'w') as writer:\n",
    "#                             json.dump(eval_output, writer)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, eval_dataset, model, tokenizer, batch_size, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_task = args.task_name\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    results = {}\n",
    "    # eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n",
    "\n",
    "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    args.eval_batch_size = batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    # eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, \n",
    "                                 batch_size=args.eval_batch_size, num_workers=args.num_workers)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    all_eval_guids = []\n",
    "    all_query_ids, all_doc_ids = None, None\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        eval_guids = batch['guid']\n",
    "        # batch = tuple(t.to(args.device) for t in batch)\n",
    "        batch = {k: v.to(args.device) for k, v in batch.items() if k != 'guid'}\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch['input_ids'],\n",
    "                      'attention_mask': batch['input_mask'],\n",
    "                      'token_type_ids': batch['segment_ids'],\n",
    "                      'labels':         batch['ranker_label_ids']}\n",
    "            if args.model_type == 'hier':\n",
    "                inputs['hier_mask'] = batch['hier_mask']\n",
    "            elif args.model_type in ['ba_bert', 'hier_att', 'ba_hier_att']:\n",
    "                inputs['hier_mask'] = batch['hier_mask']\n",
    "                inputs['behavior_rel_pos_mask'] = batch['behavior_rel_pos_mask']\n",
    "                inputs['behavior_type_mask'] = batch['behavior_type_mask']\n",
    "                \n",
    "                \n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "                       \n",
    "            if args.local_rank not in [-1, 0]:\n",
    "                gather_logits = [torch.ones_like(logits) for _ in range(torch.distributed.get_world_size())]\n",
    "                torch.distributed.all_gather(gather_logits, logits)\n",
    "                \n",
    "                query_ids = batch['query_id']                \n",
    "                gather_query_ids = [torch.ones_like(query_ids) for _ in range(torch.distributed.get_world_size())]                \n",
    "                torch.distributed.all_gather(gather_query_ids, query_ids)\n",
    "                \n",
    "                doc_ids = batch['doc_id']\n",
    "                gather_doc_ids = [torch.ones_like(doc_ids) for _ in range(torch.distributed.get_world_size())]\n",
    "                torch.distributed.all_gather(gather_doc_ids, doc_ids)              \n",
    "\n",
    "                label_ids = inputs['labels']\n",
    "                gather_label_ids = [torch.ones_like(label_ids) for _ in range(torch.distributed.get_world_size())]\n",
    "                torch.distributed.all_gather(gather_label_ids, label_ids)\n",
    "            else:\n",
    "                all_eval_guids.extend(eval_guids)\n",
    "            \n",
    "\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            if args.local_rank in [-1, 0]:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "                out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = gather_logits.detach().cpu().numpy()\n",
    "                out_label_ids = gather_label_ids.detach().cpu().numpy()\n",
    "                all_query_ids = gather_query_ids.detach().cpu().numpy()\n",
    "                all_doc_ids = gather_doc_ids.detach().cpu().numpy()\n",
    "                \n",
    "        else:\n",
    "            if args.local_rank in [-1, 0]:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "            else:\n",
    "                preds = np.append(preds, gather_logits.detach().cpu().numpy(), axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, gather_label_ids.detach().cpu().numpy(), axis=0)\n",
    "                all_query_ids = np.append(all_query_ids, gather_query_ids.detach().cpu().numpy(), axis=0)\n",
    "                all_doc_ids = np.append(all_doc_ids, gather_doc_ids.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    if args.output_mode == \"classification\":\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "    elif args.output_mode == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    elif args.output_mode == \"ranking\":\n",
    "        preds = softmax(preds, axis=1)\n",
    "        # print(preds)\n",
    "        preds = np.squeeze(preds[:, 1])\n",
    "        # print(preds)\n",
    "        \n",
    "    # print(out_label_ids)\n",
    "    # print(all_eval_guids)\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        result, qrels, run = compute_metrics(eval_task, preds, out_label_ids, guids=all_eval_guids)\n",
    "    else:\n",
    "        result, qrels, run = compute_metrics(eval_task, preds, out_label_ids, \n",
    "                                             query_ids=all_query_ids, doc_ids=all_doc_ids)\n",
    "    results.update(result)\n",
    "    eval_output = {'qrels': qrels,\n",
    "                  'run': run,\n",
    "                  'ranker_test_all_label_ids': out_label_ids.tolist(),\n",
    "                  'guids': all_eval_guids,\n",
    "                  'preds': preds.tolist()}\n",
    "\n",
    "    # output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "    # with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        # writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return results, eval_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/26/2019 16:23:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "09/26/2019 16:23:00 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /mnt/scratch/chenqu/huggingface/config.json\n",
      "09/26/2019 16:23:00 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"stateful_search\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "09/26/2019 16:23:00 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /net/home/chenqu/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "09/26/2019 16:23:00 - INFO - pytorch_transformers.tokenization_utils -   Adding [EMPTY_QUERY] to the vocabulary\n",
      "09/26/2019 16:23:00 - INFO - pytorch_transformers.tokenization_utils -   Adding [EMPTY_TITLE] to the vocabulary\n",
      "09/26/2019 16:23:00 - INFO - pytorch_transformers.modeling_utils -   loading weights file /mnt/scratch/chenqu/huggingface/pytorch_model.bin\n",
      "09/26/2019 16:23:04 - INFO - pytorch_transformers.modeling_utils -   Weights of BehaviorAwareHierAttBertConcatForStatefulSearch not initialized from pretrained model: ['classifier.weight', 'classifier.bias', 'behavior_att.attention.self.query.weight', 'behavior_att.attention.self.query.bias', 'behavior_att.attention.self.key.weight', 'behavior_att.attention.self.key.bias', 'behavior_att.attention.self.value.weight', 'behavior_att.attention.self.value.bias', 'behavior_att.attention.output.dense.weight', 'behavior_att.attention.output.dense.bias', 'behavior_att.attention.output.LayerNorm.weight', 'behavior_att.attention.output.LayerNorm.bias', 'behavior_att.intermediate.dense.weight', 'behavior_att.intermediate.dense.bias', 'behavior_att.output.dense.weight', 'behavior_att.output.dense.bias', 'behavior_att.output.LayerNorm.weight', 'behavior_att.output.LayerNorm.bias', 'sess_att.attention.self.query.weight', 'sess_att.attention.self.query.bias', 'sess_att.attention.self.key.weight', 'sess_att.attention.self.key.bias', 'sess_att.attention.self.value.weight', 'sess_att.attention.self.value.bias', 'sess_att.attention.output.dense.weight', 'sess_att.attention.output.dense.bias', 'sess_att.attention.output.LayerNorm.weight', 'sess_att.attention.output.LayerNorm.bias', 'sess_att.intermediate.dense.weight', 'sess_att.intermediate.dense.bias', 'sess_att.output.dense.weight', 'sess_att.output.dense.bias', 'sess_att.output.LayerNorm.weight', 'sess_att.output.LayerNorm.bias', 'LayerNorm.weight', 'LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "09/26/2019 16:23:04 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BehaviorAwareHierAttBertConcatForStatefulSearch: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "09/26/2019 16:23:08 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/mnt/scratch/chenqu/aol/preprocessed/', dataset='aol', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, enable_component_embeddings=True, enable_turn_id_embeddings=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, history_num=1, include_skipped=True, learning_rate=0.0001, load_small=True, local_rank=-1, logging_steps=5, max_grad_norm=1.0, max_seq_length=32, max_steps=-1, model_name_or_path='/mnt/scratch/chenqu/huggingface/', model_type='ba_hier_att', n_gpu=1, no_cuda=False, num_train_epochs=3.0, num_workers=0, output_dir='/mnt/scratch/chenqu/stateful_search/ba_hier_att/', output_mode='ranking', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=192, per_gpu_test_batch_size=24, per_gpu_train_batch_size=8, save_steps=5, seed=42, server_ip='', server_port='', task_name='stateful_search', tokenizer_name='bert-base-uncased', warmup_portion=0.1, warmup_steps=0, weight_decay=0.0)\n",
      "09/26/2019 16:23:08 - INFO - utils -   processing aol data\n",
      "09/26/2019 16:23:08 - INFO - utils -   processing aol data\n",
      "09/26/2019 16:23:08 - INFO - utils -   processing aol data\n",
      "09/26/2019 16:23:08 - INFO - __main__ -   ***** Running training *****\n",
      "09/26/2019 16:23:08 - INFO - __main__ -     Num examples = 100\n",
      "09/26/2019 16:23:08 - INFO - __main__ -     Num Epochs = 3\n",
      "09/26/2019 16:23:08 - INFO - __main__ -     Instantaneous batch size per GPU = 8\n",
      "09/26/2019 16:23:08 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "09/26/2019 16:23:08 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "09/26/2019 16:23:08 - INFO - __main__ -     Total optimization steps = 39\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 1/13 [00:02<00:32,  2.71s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 2/13 [00:02<00:21,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 3/13 [00:03<00:14,  1.41s/it]\u001b[A\n",
      "Iteration:  31%|███       | 4/13 [00:03<00:09,  1.03s/it]\u001b[A09/26/2019 16:23:11 - INFO - __main__ -   Saving model checkpoint to /mnt/scratch/chenqu/stateful_search/ba_hier_att/checkpoint-5\n",
      "\n",
      "Iteration:  38%|███▊      | 5/13 [00:03<00:06,  1.15it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 6/13 [00:03<00:04,  1.54it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 7/13 [00:03<00:03,  1.98it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 8/13 [00:04<00:01,  2.51it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 9/13 [00:04<00:01,  3.05it/s]\u001b[A09/26/2019 16:23:13 - INFO - __main__ -   Saving model checkpoint to /mnt/scratch/chenqu/stateful_search/ba_hier_att/checkpoint-10\n",
      "\n",
      "Iteration:  77%|███████▋  | 10/13 [00:04<00:01,  2.46it/s]\u001b[A\n",
      "Iteration:  85%|████████▍ | 11/13 [00:05<00:00,  3.01it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 12/13 [00:05<00:00,  3.60it/s]\u001b[A\n",
      "Epoch:  33%|███▎      | 1/3 [00:05<00:10,  5.32s/it]2it/s]\u001b[A\n",
      "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 1/13 [00:00<00:01,  6.98it/s]\u001b[A09/26/2019 16:23:14 - INFO - __main__ -   Saving model checkpoint to /mnt/scratch/chenqu/stateful_search/ba_hier_att/checkpoint-15\n",
      "\n",
      "Iteration:  15%|█▌        | 2/13 [00:00<00:02,  4.18it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 3/13 [00:00<00:02,  4.67it/s]\u001b[A\n",
      "Iteration:  31%|███       | 4/13 [00:00<00:01,  5.24it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 5/13 [00:01<00:01,  5.73it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 6/13 [00:01<00:01,  5.77it/s]\u001b[A09/26/2019 16:23:15 - INFO - __main__ -   Saving model checkpoint to /mnt/scratch/chenqu/stateful_search/ba_hier_att/checkpoint-20\n",
      "\n",
      "Iteration:  54%|█████▍    | 7/13 [00:01<00:01,  3.42it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 8/13 [00:01<00:01,  3.92it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 9/13 [00:02<00:00,  4.39it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 10/13 [00:02<00:00,  4.92it/s]\u001b[A\n",
      "Iteration:  85%|████████▍ | 11/13 [00:02<00:00,  5.46it/s]\u001b[A09/26/2019 16:23:16 - INFO - __main__ -   Saving model checkpoint to /mnt/scratch/chenqu/stateful_search/ba_hier_att/checkpoint-25\n",
      "\n",
      "Iteration:  92%|█████████▏| 12/13 [00:02<00:00,  3.88it/s]\u001b[A\n",
      "Epoch:  67%|██████▋   | 2/3 [00:08<00:04,  4.61s/it]3it/s]\u001b[A\n",
      "Iteration:   0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 1/13 [00:00<00:01,  7.28it/s]\u001b[A\n",
      "Iteration:  15%|█▌        | 2/13 [00:00<00:01,  7.18it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 3/13 [00:00<00:01,  7.12it/s]\u001b[A09/26/2019 16:23:17 - INFO - __main__ -   Saving model checkpoint to /mnt/scratch/chenqu/stateful_search/ba_hier_att/checkpoint-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  31%|███       | 4/13 [00:00<00:02,  4.29it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 5/13 [00:01<00:01,  4.81it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 6/13 [00:01<00:01,  5.25it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 7/13 [00:01<00:01,  5.65it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 8/13 [00:01<00:00,  5.96it/s]\u001b[A09/26/2019 16:23:18 - INFO - __main__ -   Saving model checkpoint to /mnt/scratch/chenqu/stateful_search/ba_hier_att/checkpoint-35\n",
      "\n",
      "Iteration:  69%|██████▉   | 9/13 [00:02<00:01,  3.55it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 10/13 [00:02<00:00,  4.19it/s]\u001b[A\n",
      "Iteration:  85%|████████▍ | 11/13 [00:02<00:00,  4.79it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 12/13 [00:02<00:00,  5.35it/s]\u001b[A\n",
      "Epoch: 100%|██████████| 3/3 [00:10<00:00,  4.00s/it]3it/s]\u001b[A\n",
      "09/26/2019 16:23:19 - INFO - __main__ -    global_step = 39, average loss = 0.5913596741664104\n",
      "09/26/2019 16:23:19 - INFO - __main__ -   Eval on all checkpoints with dev set\n",
      "09/26/2019 16:23:19 - INFO - pytorch_transformers.tokenization_utils -   Model name '/mnt/scratch/chenqu/stateful_search/ba_hier_att/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/mnt/scratch/chenqu/stateful_search/ba_hier_att/' is a path or url to a directory containing tokenizer files.\n",
      "09/26/2019 16:23:19 - INFO - pytorch_transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/stateful_search/ba_hier_att/vocab.txt\n",
      "09/26/2019 16:23:19 - INFO - pytorch_transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/stateful_search/ba_hier_att/added_tokens.json\n",
      "09/26/2019 16:23:19 - INFO - pytorch_transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/stateful_search/ba_hier_att/special_tokens_map.json\n",
      "09/26/2019 16:23:19 - INFO - pytorch_transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/stateful_search/ba_hier_att/tokenizer_config.json\n",
      "09/26/2019 16:23:19 - INFO - __main__ -   Evaluate the following checkpoints: ['/mnt/scratch/chenqu/stateful_search/ba_hier_att/checkpoint-10', '/mnt/scratch/chenqu/stateful_search/ba_hier_att/checkpoint-15', '/mnt/scratch/chenqu/stateful_search/ba_hier_att/checkpoint-20', '/mnt/scratch/chenqu/stateful_search/ba_hier_att/checkpoint-25', '/mnt/scratch/chenqu/stateful_search/ba_hier_att/checkpoint-30', '/mnt/scratch/chenqu/stateful_search/ba_hier_att/checkpoint-35', '/mnt/scratch/chenqu/stateful_search/ba_hier_att/checkpoint-5']\n",
      "09/26/2019 16:23:22 - INFO - __main__ -   ***** Running evaluation 10 *****\n",
      "09/26/2019 16:23:22 - INFO - __main__ -     Num examples = 100\n",
      "09/26/2019 16:23:22 - INFO - __main__ -     Batch size = 192\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\n",
      "09/26/2019 16:23:23 - INFO - __main__ -   ***** Eval results 10 *****\n",
      "09/26/2019 16:23:23 - INFO - __main__ -     mrr = 0.6383333333333333\n",
      "09/26/2019 16:23:23 - INFO - __main__ -     ndcg = 0.7197987728835498\n",
      "09/26/2019 16:23:23 - INFO - __main__ -     ndcg_10 = 0.7197987728835498\n",
      "09/26/2019 16:23:27 - INFO - __main__ -   ***** Running evaluation 15 *****\n",
      "09/26/2019 16:23:27 - INFO - __main__ -     Num examples = 100\n",
      "09/26/2019 16:23:27 - INFO - __main__ -     Batch size = 192\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  2.92it/s]\n",
      "09/26/2019 16:23:27 - INFO - __main__ -   ***** Eval results 15 *****\n",
      "09/26/2019 16:23:27 - INFO - __main__ -     mrr = 0.6508333333333333\n",
      "09/26/2019 16:23:27 - INFO - __main__ -     ndcg = 0.7379109456109373\n",
      "09/26/2019 16:23:27 - INFO - __main__ -     ndcg_10 = 0.7379109456109373\n",
      "09/26/2019 16:23:31 - INFO - __main__ -   ***** Running evaluation 20 *****\n",
      "09/26/2019 16:23:31 - INFO - __main__ -     Num examples = 100\n",
      "09/26/2019 16:23:31 - INFO - __main__ -     Batch size = 192\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  2.90it/s]\n",
      "09/26/2019 16:23:31 - INFO - __main__ -   ***** Eval results 20 *****\n",
      "09/26/2019 16:23:31 - INFO - __main__ -     mrr = 0.5766666666666668\n",
      "09/26/2019 16:23:31 - INFO - __main__ -     ndcg = 0.6854242753766778\n",
      "09/26/2019 16:23:31 - INFO - __main__ -     ndcg_10 = 0.6854242753766778\n",
      "09/26/2019 16:23:35 - INFO - __main__ -   ***** Running evaluation 25 *****\n",
      "09/26/2019 16:23:35 - INFO - __main__ -     Num examples = 100\n",
      "09/26/2019 16:23:35 - INFO - __main__ -     Batch size = 192\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\n",
      "09/26/2019 16:23:35 - INFO - __main__ -   ***** Eval results 25 *****\n",
      "09/26/2019 16:23:35 - INFO - __main__ -     mrr = 0.5700000000000001\n",
      "09/26/2019 16:23:35 - INFO - __main__ -     ndcg = 0.6811078141442283\n",
      "09/26/2019 16:23:35 - INFO - __main__ -     ndcg_10 = 0.6811078141442283\n",
      "09/26/2019 16:23:39 - INFO - __main__ -   ***** Running evaluation 30 *****\n",
      "09/26/2019 16:23:39 - INFO - __main__ -     Num examples = 100\n",
      "09/26/2019 16:23:39 - INFO - __main__ -     Batch size = 192\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  2.89it/s]\n",
      "09/26/2019 16:23:39 - INFO - __main__ -   ***** Eval results 30 *****\n",
      "09/26/2019 16:23:39 - INFO - __main__ -     mrr = 0.6058333333333333\n",
      "09/26/2019 16:23:39 - INFO - __main__ -     ndcg = 0.7073439597944353\n",
      "09/26/2019 16:23:39 - INFO - __main__ -     ndcg_10 = 0.7073439597944353\n",
      "09/26/2019 16:23:42 - INFO - __main__ -   ***** Running evaluation 35 *****\n",
      "09/26/2019 16:23:42 - INFO - __main__ -     Num examples = 100\n",
      "09/26/2019 16:23:42 - INFO - __main__ -     Batch size = 192\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  2.92it/s]\n",
      "09/26/2019 16:23:43 - INFO - __main__ -   ***** Eval results 35 *****\n",
      "09/26/2019 16:23:43 - INFO - __main__ -     mrr = 0.5958333333333334\n",
      "09/26/2019 16:23:43 - INFO - __main__ -     ndcg = 0.6995224875614747\n",
      "09/26/2019 16:23:43 - INFO - __main__ -     ndcg_10 = 0.6995224875614747\n",
      "09/26/2019 16:23:46 - INFO - __main__ -   ***** Running evaluation 5 *****\n",
      "09/26/2019 16:23:46 - INFO - __main__ -     Num examples = 100\n",
      "09/26/2019 16:23:46 - INFO - __main__ -     Batch size = 192\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  2.90it/s]\n",
      "09/26/2019 16:23:47 - INFO - __main__ -   ***** Eval results 5 *****\n",
      "09/26/2019 16:23:47 - INFO - __main__ -     mrr = 0.4341666666666667\n",
      "09/26/2019 16:23:47 - INFO - __main__ -     ndcg = 0.5744321137397037\n",
      "09/26/2019 16:23:47 - INFO - __main__ -     ndcg_10 = 0.5744321137397037\n",
      "09/26/2019 16:23:47 - INFO - pytorch_transformers.tokenization_utils -   Model name '/mnt/scratch/chenqu/stateful_search/ba_hier_att/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/mnt/scratch/chenqu/stateful_search/ba_hier_att/' is a path or url to a directory containing tokenizer files.\n",
      "09/26/2019 16:23:47 - INFO - pytorch_transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/stateful_search/ba_hier_att/vocab.txt\n",
      "09/26/2019 16:23:47 - INFO - pytorch_transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/stateful_search/ba_hier_att/added_tokens.json\n",
      "09/26/2019 16:23:47 - INFO - pytorch_transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/stateful_search/ba_hier_att/special_tokens_map.json\n",
      "09/26/2019 16:23:47 - INFO - pytorch_transformers.tokenization_utils -   loading file /mnt/scratch/chenqu/stateful_search/ba_hier_att/tokenizer_config.json\n",
      "09/26/2019 16:23:47 - INFO - __main__ -   Testing\n",
      "09/26/2019 16:23:51 - INFO - __main__ -   ***** Running evaluation test *****\n",
      "09/26/2019 16:23:51 - INFO - __main__ -     Num examples = 100\n",
      "09/26/2019 16:23:51 - INFO - __main__ -     Batch size = 24\n",
      "Evaluating: 100%|██████████| 5/5 [00:02<00:00,  1.35s/it]\n",
      "09/26/2019 16:23:54 - INFO - __main__ -   ***** Eval results test *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/26/2019 16:23:54 - INFO - __main__ -     mrr = 0.045\n",
      "09/26/2019 16:23:54 - INFO - __main__ -     ndcg = 0.22020815112515807\n",
      "09/26/2019 16:23:54 - INFO - __main__ -     ndcg_10 = 0.0\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "## Required parameters\n",
    "parser.add_argument(\"--data_dir\", default='/mnt/scratch/chenqu/aol/preprocessed/', type=str, required=False,\n",
    "                    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "parser.add_argument(\"--model_type\", default='ba_hier_att', type=str, required=False,\n",
    "                    help=\"Model type selected in the list: [bert, ba_bert, hier, hier_att, ba_hier_att]\" )\n",
    "parser.add_argument(\"--model_name_or_path\", default='/mnt/scratch/chenqu/huggingface/', type=str, required=False,\n",
    "                    help=\"Path to pre-trained model or shortcut name\")\n",
    "parser.add_argument(\"--task_name\", default='stateful_search', type=str, required=False,\n",
    "                    help=\"The name of the task to train\")\n",
    "parser.add_argument(\"--output_dir\", default='/mnt/scratch/chenqu/stateful_search/ba_hier_att/', type=str, required=False,\n",
    "                    help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "\n",
    "## Other parameters\n",
    "parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                    help=\"Pretrained config name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--tokenizer_name\", default=\"bert-base-uncased\", type=str,\n",
    "                    help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n",
    "                    help=\"Where do you want to store the pre-trained models downloaded from s3\")\n",
    "parser.add_argument(\"--max_seq_length\", default=32, type=int,\n",
    "                    help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                         \"than this will be truncated, sequences shorter will be padded.\")\n",
    "parser.add_argument(\"--do_train\", default=True, type=str2bool,\n",
    "                    help=\"Whether to run training.\")\n",
    "parser.add_argument(\"--do_eval\", default=True, type=str2bool,\n",
    "                    help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument(\"--evaluate_during_training\", default=True, type=str2bool,\n",
    "                    help=\"Run evaluation during training at each logging step.\")\n",
    "parser.add_argument(\"--do_lower_case\", default=True, type=str2bool,\n",
    "                    help=\"Set this flag if you are using an uncased model.\")\n",
    "\n",
    "parser.add_argument(\"--per_gpu_train_batch_size\", default=8, type=int,\n",
    "                    help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\"--per_gpu_eval_batch_size\", default=192, type=int,\n",
    "                    help=\"Batch size per GPU/CPU for evaluation.\")\n",
    "parser.add_argument(\"--per_gpu_test_batch_size\", default=24, type=int,\n",
    "                    help=\"Batch size per GPU/CPU for testing.\")\n",
    "parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument(\"--learning_rate\", default=1e-4, type=float,\n",
    "                    help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
    "                    help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
    "                    help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
    "                    help=\"Max gradient norm.\")\n",
    "parser.add_argument(\"--num_train_epochs\", default=3.0, type=float,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
    "                    help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
    "parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
    "                    help=\"Linear warmup over warmup_steps.\")\n",
    "\n",
    "parser.add_argument('--logging_steps', type=int, default=5,\n",
    "                    help=\"Log and save checkpoint every X updates steps.\")\n",
    "parser.add_argument('--save_steps', type=int, default=5,\n",
    "                    help=\"Save checkpoint every X updates steps, this is disabled in our code\")\n",
    "parser.add_argument(\"--eval_all_checkpoints\", default=False, type=str2bool,\n",
    "                    help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\")\n",
    "parser.add_argument(\"--no_cuda\", default=False, type=str2bool,\n",
    "                    help=\"Avoid using CUDA when available\")\n",
    "parser.add_argument('--overwrite_output_dir', default=True, type=str2bool,\n",
    "                    help=\"Overwrite the content of the output directory\")\n",
    "parser.add_argument('--overwrite_cache', action='store_true',\n",
    "                    help=\"Overwrite the cached training and evaluation sets\")\n",
    "parser.add_argument('--seed', type=int, default=42,\n",
    "                    help=\"random seed for initialization\")\n",
    "\n",
    "parser.add_argument('--fp16', default=False, type=str2bool,\n",
    "                    help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
    "parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
    "                    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                         \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
    "                    help=\"For distributed training: local_rank\")\n",
    "parser.add_argument('--server_ip', type=str, default='', help=\"For distant debugging.\")\n",
    "parser.add_argument('--server_port', type=str, default='', help=\"For distant debugging.\")\n",
    "\n",
    "# parameters we added\n",
    "parser.add_argument(\"--include_skipped\", default=True, type=str2bool, required=False,\n",
    "                    help=\"whether to include the skipped doc from prev turn\")\n",
    "parser.add_argument(\"--enable_turn_id_embeddings\", default=True, type=str2bool, required=False,\n",
    "                    help=\"whether to enable turn id embeddings\")\n",
    "parser.add_argument(\"--enable_component_embeddings\", default=True, type=str2bool, required=False,\n",
    "                    help=\"whether to enable component embeddings\")\n",
    "parser.add_argument(\"--load_small\", default=True, type=str2bool, required=False,\n",
    "                    help=\"whether to just a small portion of data during development\")\n",
    "parser.add_argument(\"--dataset\", default='aol', type=str, required=False,\n",
    "                    help=\"aol or msmarco. For bing data, we do not use the first query in a session\")\n",
    "parser.add_argument(\"--history_num\", default=1, type=int, required=False,\n",
    "                    help=\"number of history turns to concat\")\n",
    "parser.add_argument(\"--num_workers\", default=0, type=int, required=False,\n",
    "                    help=\"number of workers for dataloader\")\n",
    "parser.add_argument(\"--warmup_portion\", default=0.1, type=float,\n",
    "                    help=\"Linear warmup over warmup_steps (=t_total * warmup_portion). override warmup_steps \")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args.output_dir))\n",
    "\n",
    "# Setup distant debugging if needed\n",
    "if args.server_ip and args.server_port:\n",
    "    # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "    import ptvsd\n",
    "    print(\"Waiting for debugger attach\")\n",
    "    ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "    ptvsd.wait_for_attach()\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "                args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
    "\n",
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "args.task_name = args.task_name.lower()\n",
    "# if args.task_name not in processors:\n",
    "#     raise ValueError(\"Task not found: %s\" % (args.task_name))\n",
    "# processor = processors[args.task_name]()\n",
    "args.output_mode = output_modes[args.task_name]\n",
    "label_list = [\"False\", \"True\"]\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "args.model_type = args.model_type.lower()\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path, num_labels=num_labels, finetuning_task=args.task_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "tokenizer.add_tokens(['[EMPTY_QUERY]', '[EMPTY_TITLE]'])\n",
    "model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "if args.model_type == 'hier':\n",
    "    for layer in model.bert.encoder.layer:\n",
    "        layer.hier.att.attention.load_state_dict(layer.attention.state_dict())\n",
    "        layer.hier.att.intermediate.load_state_dict(layer.intermediate.state_dict())\n",
    "        layer.hier.att.output.load_state_dict(layer.output.state_dict())\n",
    "\n",
    "\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "\n",
    "# Training\n",
    "if args.do_train:\n",
    "    # train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n",
    "    train_dataset = ConcatModelDataset(os.path.join(args.data_dir, \"session_train.txt\"), args.include_skipped,\n",
    "                               args.max_seq_length, tokenizer, args.output_mode, args.load_small, args.dataset,\n",
    "                                 args.history_num)\n",
    "    eval_dataset = ConcatModelDataset(os.path.join(args.data_dir, \"session_dev_small.txt\"), args.include_skipped, \n",
    "                                args.max_seq_length, tokenizer, args.output_mode, args.load_small, args.dataset,\n",
    "                                 args.history_num)\n",
    "    test_dataset = ConcatModelDataset(os.path.join(args.data_dir, \"session_test.txt\"), args.include_skipped, \n",
    "                                args.max_seq_length, tokenizer, args.output_mode, args.load_small, args.dataset,\n",
    "                                 args.history_num)\n",
    "    global_step, tr_loss = train(args, train_dataset, eval_dataset, model, tokenizer)\n",
    "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "    \n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "    \n",
    "if not args.do_train and args.do_eval:\n",
    "    eval_dataset = ConcatModelDataset(os.path.join(args.data_dir, \"session_dev_small.txt\"), args.include_skipped, \n",
    "                                args.max_seq_length, tokenizer, args.output_mode, args.load_small, args.dataset,\n",
    "                                 args.history_num)\n",
    "    test_dataset = ConcatModelDataset(os.path.join(args.data_dir, \"session_test.txt\"), args.include_skipped, \n",
    "                                args.max_seq_length, tokenizer, args.output_mode, args.load_small, args.dataset,\n",
    "                                 args.history_num)\n",
    "\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "#     if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "#         # Create output directory if needed\n",
    "#         if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "#             os.makedirs(args.output_dir)\n",
    "\n",
    "#         logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "#         # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "#         # They can then be reloaded using `from_pretrained()`\n",
    "#         model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "#         model_to_save.save_pretrained(args.output_dir)\n",
    "#         tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "#         # Good practice: save your training arguments together with the trained model\n",
    "#         torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n",
    "\n",
    "#         # Load a trained model and vocabulary that you have fine-tuned\n",
    "#         model = model_class.from_pretrained(args.output_dir)\n",
    "#         tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "#         model.to(args.device)\n",
    "\n",
    "best_eval_mrr = 0.0\n",
    "best_global_step = 0\n",
    "results = {}\n",
    "if args.do_eval and args.local_rank in [-1, 0]:\n",
    "    logger.info(\"Eval on all checkpoints with dev set\")\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "    checkpoints = [args.output_dir]\n",
    "    checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "    logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "    logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "    for checkpoint in checkpoints:\n",
    "        global_step = checkpoint.split('-')[-1]\n",
    "        model = model_class.from_pretrained(checkpoint)\n",
    "        model.to(args.device)\n",
    "        result, eval_output = evaluate(args, eval_dataset, model, tokenizer, \n",
    "                                       args.per_gpu_eval_batch_size, prefix=global_step)\n",
    "        if result['mrr'] > best_eval_mrr:\n",
    "            best_global_step = global_step\n",
    "            best_eval_mrr = result['mrr']\n",
    "        \n",
    "        result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
    "        results.update(result)       \n",
    "\n",
    "        output_eval_preds_file = os.path.join(args.output_dir, \"eval_preds_{}.txt\".format(global_step))\n",
    "        with open(output_eval_preds_file, 'w') as writer:\n",
    "            json.dump(eval_output, writer)\n",
    "            \n",
    "    results['best_global_step'] = best_global_step\n",
    "    output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        for key in sorted(results.keys()):\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(results[key])))\n",
    "\n",
    "\n",
    "# Evaluation on test set\n",
    "if args.do_eval and args.local_rank in [-1, 0]:\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "    logger.info(\"Testing\")\n",
    "    model = model_class.from_pretrained(os.path.join(args.output_dir, 'checkpoint-{}'.format(best_global_step)))\n",
    "    model.to(args.device)\n",
    "    result, test_output = evaluate(args, test_dataset, model, \n",
    "                                   tokenizer, args.per_gpu_test_batch_size, prefix='test')\n",
    "    result = dict((k + '_{}'.format('test'), v) for k, v in result.items())\n",
    "    results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(args.output_dir, \"test_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        for key in sorted(results.keys()):\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(results[key])))\n",
    "\n",
    "    output_test_preds_file = os.path.join(args.output_dir, \"test_preds.txt\")\n",
    "    with open(output_test_preds_file, 'w') as writer:\n",
    "        json.dump(test_output, writer)\n",
    "\n",
    "    # return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
